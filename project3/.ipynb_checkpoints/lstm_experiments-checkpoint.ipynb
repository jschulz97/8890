{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, outputs):\n",
    "        super(myLSTM, self).__init__()\n",
    "\n",
    "        self.model = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "        )\n",
    "\n",
    "        self.out = nn.Linear(hidden_size, outputs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape (batch, time_step, input_size)\n",
    "        # r_out shape (batch, time_step, output_size)\n",
    "        # h_n shape (n_layers, batch, hidden_size)\n",
    "        # h_c shape (n_layers, batch, hidden_size)\n",
    "        r_out, (h_n, h_c) = self.model(x, None)   # None represents zero initial hidden state\n",
    "\n",
    "        # choose r_out at the last time step\n",
    "        out = self.out(r_out[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Dataset (Numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Hyper parameters\n",
    "num_epochs = 2000\n",
    "num_classes = 10\n",
    "batch_size = 10\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "numbers_train_forwards = [0,1,2,3,4,5,6,7,8,9,0]\n",
    "numbers_train_backwards = [0,9,8,7,6,5,4,3,2,1,0]\n",
    "numbers_train_i = [0,1,2,3,4,5,6,7,8,9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/2000], Loss: 1.4593\n",
      "Epoch [200/2000], Loss: 0.9514\n",
      "Epoch [300/2000], Loss: 0.7341\n",
      "Epoch [400/2000], Loss: 0.6174\n",
      "Epoch [500/2000], Loss: 0.5493\n",
      "Epoch [600/2000], Loss: 0.5062\n",
      "Epoch [700/2000], Loss: 0.4744\n",
      "Epoch [800/2000], Loss: 0.4406\n",
      "Epoch [900/2000], Loss: 0.3719\n",
      "Epoch [1000/2000], Loss: 0.2360\n",
      "Epoch [1100/2000], Loss: 0.1783\n",
      "Epoch [1200/2000], Loss: 0.1487\n",
      "Epoch [1300/2000], Loss: 0.1240\n",
      "Epoch [1400/2000], Loss: 0.1023\n",
      "Epoch [1500/2000], Loss: 0.0837\n",
      "Epoch [1600/2000], Loss: 0.0682\n",
      "Epoch [1700/2000], Loss: 0.0554\n",
      "Epoch [1800/2000], Loss: 0.0448\n",
      "Epoch [1900/2000], Loss: 0.0360\n",
      "Epoch [2000/2000], Loss: 0.0287\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "import random\n",
    "\n",
    "# Create model\n",
    "net = myLSTM(5, 8, 1, 10)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    for i in numbers_train_i[0:5]:\n",
    "        ## Forward numbers\n",
    "        input_tensor = torch.Tensor([numbers_train_forwards[i:i+5]]).unsqueeze(0)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = net(input_tensor)\n",
    "\n",
    "        label = torch.LongTensor([numbers_train_forwards[i+5]])\n",
    "        loss = criterion(outputs, label)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step() \n",
    "        \n",
    "        ## Backward numbers\n",
    "        input_tensor = torch.Tensor([numbers_train_backwards[i:i+5]]).unsqueeze(0)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = net(input_tensor)\n",
    "\n",
    "        label = torch.LongTensor([numbers_train_backwards[i+5]])\n",
    "        loss = criterion(outputs, label)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if (epoch+1) % 100 == 0:\n",
    "        print ('Epoch [{}/{}], Loss: {:.4f}' \n",
    "                   .format(epoch+1, num_epochs, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FORWARDS\n",
      "input: [[[0. 1. 2. 3. 4.]]] output: tensor([5])\n",
      "input: [[[1. 2. 3. 4. 5.]]] output: tensor([6])\n",
      "input: [[[2. 3. 4. 5. 6.]]] output: tensor([7])\n",
      "input: [[[3. 4. 5. 6. 7.]]] output: tensor([8])\n",
      "input: [[[4. 5. 6. 7. 8.]]] output: tensor([9])\n",
      "BACKWARDS\n",
      "input: [[[0. 9. 8. 7. 6.]]] output: tensor([5])\n",
      "input: [[[9. 8. 7. 6. 5.]]] output: tensor([4])\n",
      "input: [[[8. 7. 6. 5. 4.]]] output: tensor([3])\n",
      "input: [[[7. 6. 5. 4. 3.]]] output: tensor([2])\n",
      "input: [[[6. 5. 4. 3. 2.]]] output: tensor([1])\n"
     ]
    }
   ],
   "source": [
    "# test_set = [[0],[9],[8],[7],[6]]\n",
    "\n",
    "# Test the model\n",
    "net.eval()  # eval mode (batchnorm uses moving mean/variance instead of mini-batch mean/variance)\n",
    "with torch.no_grad():\n",
    "    print('FORWARDS')\n",
    "    for i in numbers_train_i[0:5]:\n",
    "        ## Forward numbers\n",
    "        input_tensor = torch.Tensor([numbers_train_forwards[i:i+5]]).unsqueeze(0)\n",
    "\n",
    "        outputs = net(input_tensor)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        print('input:', np.array(input_tensor), 'output:', predicted)\n",
    "    \n",
    "    print('BACKWARDS')\n",
    "    for i in numbers_train_i[0:5]:\n",
    "        ## Forward numbers\n",
    "        input_tensor = torch.Tensor([numbers_train_backwards[i:i+5]]).unsqueeze(0)\n",
    "\n",
    "        outputs = net(input_tensor)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        print('input:', np.array(input_tensor), 'output:', predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: 0 output: tensor([1])\n",
      "input: 9 output: tensor([0])\n",
      "input: 8 output: tensor([8])\n",
      "input: 7 output: tensor([7])\n",
      "input: 6 output: tensor([6])\n",
      "input: 5 output: tensor([5])\n",
      "input: 4 output: tensor([3])\n",
      "input: 3 output: tensor([3])\n",
      "input: 2 output: tensor([2])\n",
      "input: 1 output: tensor([0])\n"
     ]
    }
   ],
   "source": [
    "# test_set = [0,1,2,3,4,5,6,7,8,9]\n",
    "test_set = [0,9,8,7,6,5,4,3,2,1]\n",
    "\n",
    "# Test the model\n",
    "net.eval()  # eval mode (batchnorm uses moving mean/variance instead of mini-batch mean/variance)\n",
    "with torch.no_grad():\n",
    "    for test_num in test_set:\n",
    "        outputs = net(torch.Tensor([test_num]).unsqueeze(0).unsqueeze(0))\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        print('input:', test_num, 'output:', predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-6c063a99f4b9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mcorrect\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mtotal\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m         \u001b[0mimages\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'test_loader' is not defined"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "net.eval()  # eval mode (batchnorm uses moving mean/variance instead of mini-batch mean/variance)\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Test Accuracy of the model on the 10000 test images: {} %'.format(100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Hyper parameters\n",
    "num_epochs = 4\n",
    "num_classes = 10\n",
    "batch_size = 100\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST dataset\n",
    "# new_mirror = 'https://ossci-datasets.s3.amazonaws.com/mnist'\n",
    "# torchvision.datasets.MNIST.resources = [\n",
    "#    ('/'.join([new_mirror, url.split('/')[-1]]), md5)\n",
    "#    for url, md5 in torchvision.datasets.MNIST.resources\n",
    "# ]\n",
    "\n",
    "train_dataset = torchvision.datasets.MNIST(root='data',\n",
    "                                           train=True, \n",
    "                                           transform=transforms.ToTensor())\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(root='data',\n",
    "                                          train=False, \n",
    "                                          transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loader\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/4], Step [100/600], Loss: 1.2198\n",
      "Epoch [1/4], Step [200/600], Loss: 0.4813\n",
      "Epoch [1/4], Step [300/600], Loss: 0.4618\n",
      "Epoch [1/4], Step [400/600], Loss: 0.3126\n",
      "Epoch [1/4], Step [500/600], Loss: 0.2688\n",
      "Epoch [1/4], Step [600/600], Loss: 0.2162\n",
      "Epoch [2/4], Step [100/600], Loss: 0.1614\n",
      "Epoch [2/4], Step [200/600], Loss: 0.2856\n",
      "Epoch [2/4], Step [300/600], Loss: 0.1111\n",
      "Epoch [2/4], Step [400/600], Loss: 0.1906\n",
      "Epoch [2/4], Step [500/600], Loss: 0.2187\n",
      "Epoch [2/4], Step [600/600], Loss: 0.3824\n",
      "Epoch [3/4], Step [100/600], Loss: 0.2043\n",
      "Epoch [3/4], Step [200/600], Loss: 0.2303\n",
      "Epoch [3/4], Step [300/600], Loss: 0.0858\n",
      "Epoch [3/4], Step [400/600], Loss: 0.0994\n",
      "Epoch [3/4], Step [500/600], Loss: 0.1195\n",
      "Epoch [3/4], Step [600/600], Loss: 0.1317\n",
      "Epoch [4/4], Step [100/600], Loss: 0.0456\n",
      "Epoch [4/4], Step [200/600], Loss: 0.0498\n",
      "Epoch [4/4], Step [300/600], Loss: 0.0829\n",
      "Epoch [4/4], Step [400/600], Loss: 0.0844\n",
      "Epoch [4/4], Step [500/600], Loss: 0.0925\n",
      "Epoch [4/4], Step [600/600], Loss: 0.0944\n"
     ]
    }
   ],
   "source": [
    "# Create model\n",
    "net = myLSTM(28, 64, 1, 10)\n",
    "net = net.to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = net(images.squeeze(1))\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of the model on the 10000 test images: 96.58 %\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "net.eval()  # eval mode (batchnorm uses moving mean/variance instead of mini-batch mean/variance)\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = net(images.squeeze())\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Test Accuracy of the model on the 10000 test images: {} %'.format(100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6, device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test the model\n",
    "net.eval()  # eval mode (batchnorm uses moving mean/variance instead of mini-batch mean/variance)\n",
    "with torch.no_grad():\n",
    "    count = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = net(images.squeeze())\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        \n",
    "        %matplotlib inline\n",
    "        ind = 8\n",
    "        plt.imshow(np.resize(images.cpu()[ind],(28,28,1)))\n",
    "        print(predicted[ind])\n",
    "\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stock Price"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_df = pd.DataFrame()\n",
    "master_dict = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30-Jun-17</td>\n",
       "      <td>943.99</td>\n",
       "      <td>945.00</td>\n",
       "      <td>929.61</td>\n",
       "      <td>929.68</td>\n",
       "      <td>2287662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>29-Jun-17</td>\n",
       "      <td>951.35</td>\n",
       "      <td>951.66</td>\n",
       "      <td>929.60</td>\n",
       "      <td>937.82</td>\n",
       "      <td>3206674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28-Jun-17</td>\n",
       "      <td>950.66</td>\n",
       "      <td>963.24</td>\n",
       "      <td>936.16</td>\n",
       "      <td>961.01</td>\n",
       "      <td>2745568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27-Jun-17</td>\n",
       "      <td>961.60</td>\n",
       "      <td>967.22</td>\n",
       "      <td>947.09</td>\n",
       "      <td>948.09</td>\n",
       "      <td>2443602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>26-Jun-17</td>\n",
       "      <td>990.00</td>\n",
       "      <td>993.99</td>\n",
       "      <td>970.33</td>\n",
       "      <td>972.09</td>\n",
       "      <td>1517912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3140</th>\n",
       "      <td>7-Jan-05</td>\n",
       "      <td>95.42</td>\n",
       "      <td>97.22</td>\n",
       "      <td>94.48</td>\n",
       "      <td>97.02</td>\n",
       "      <td>9666175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3141</th>\n",
       "      <td>6-Jan-05</td>\n",
       "      <td>97.72</td>\n",
       "      <td>98.05</td>\n",
       "      <td>93.95</td>\n",
       "      <td>94.37</td>\n",
       "      <td>10389803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3142</th>\n",
       "      <td>5-Jan-05</td>\n",
       "      <td>96.82</td>\n",
       "      <td>98.55</td>\n",
       "      <td>96.21</td>\n",
       "      <td>96.85</td>\n",
       "      <td>8239545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3143</th>\n",
       "      <td>4-Jan-05</td>\n",
       "      <td>100.77</td>\n",
       "      <td>101.57</td>\n",
       "      <td>96.84</td>\n",
       "      <td>97.35</td>\n",
       "      <td>13762396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3144</th>\n",
       "      <td>3-Jan-05</td>\n",
       "      <td>98.80</td>\n",
       "      <td>101.92</td>\n",
       "      <td>97.83</td>\n",
       "      <td>101.46</td>\n",
       "      <td>15860692</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3145 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Date    Open    High     Low   Close    Volume\n",
       "0     30-Jun-17  943.99  945.00  929.61  929.68   2287662\n",
       "1     29-Jun-17  951.35  951.66  929.60  937.82   3206674\n",
       "2     28-Jun-17  950.66  963.24  936.16  961.01   2745568\n",
       "3     27-Jun-17  961.60  967.22  947.09  948.09   2443602\n",
       "4     26-Jun-17  990.00  993.99  970.33  972.09   1517912\n",
       "...         ...     ...     ...     ...     ...       ...\n",
       "3140   7-Jan-05   95.42   97.22   94.48   97.02   9666175\n",
       "3141   6-Jan-05   97.72   98.05   93.95   94.37  10389803\n",
       "3142   5-Jan-05   96.82   98.55   96.21   96.85   8239545\n",
       "3143   4-Jan-05  100.77  101.57   96.84   97.35  13762396\n",
       "3144   3-Jan-05   98.80  101.92   97.83  101.46  15860692\n",
       "\n",
       "[3145 rows x 6 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('google_stock_05-17.csv', sep=',', header=0, skiprows=0)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-04-29</td>\n",
       "      <td>1274.000000</td>\n",
       "      <td>1289.270020</td>\n",
       "      <td>1266.295044</td>\n",
       "      <td>1287.579956</td>\n",
       "      <td>1287.579956</td>\n",
       "      <td>2499400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-04-30</td>\n",
       "      <td>1185.000000</td>\n",
       "      <td>1192.810059</td>\n",
       "      <td>1175.000000</td>\n",
       "      <td>1188.479980</td>\n",
       "      <td>1188.479980</td>\n",
       "      <td>6207000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-05-01</td>\n",
       "      <td>1188.050049</td>\n",
       "      <td>1188.050049</td>\n",
       "      <td>1167.180054</td>\n",
       "      <td>1168.079956</td>\n",
       "      <td>1168.079956</td>\n",
       "      <td>2639200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-05-02</td>\n",
       "      <td>1167.760010</td>\n",
       "      <td>1174.189941</td>\n",
       "      <td>1155.001953</td>\n",
       "      <td>1162.609985</td>\n",
       "      <td>1162.609985</td>\n",
       "      <td>1944800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-05-03</td>\n",
       "      <td>1173.650024</td>\n",
       "      <td>1186.800049</td>\n",
       "      <td>1169.000000</td>\n",
       "      <td>1185.400024</td>\n",
       "      <td>1185.400024</td>\n",
       "      <td>1980700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>2021-04-21</td>\n",
       "      <td>2285.250000</td>\n",
       "      <td>2295.320068</td>\n",
       "      <td>2258.570068</td>\n",
       "      <td>2293.290039</td>\n",
       "      <td>2293.290039</td>\n",
       "      <td>1196500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>2021-04-22</td>\n",
       "      <td>2293.229980</td>\n",
       "      <td>2303.761963</td>\n",
       "      <td>2256.449951</td>\n",
       "      <td>2267.919922</td>\n",
       "      <td>2267.919922</td>\n",
       "      <td>1054800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>2021-04-23</td>\n",
       "      <td>2283.469971</td>\n",
       "      <td>2325.820068</td>\n",
       "      <td>2278.209961</td>\n",
       "      <td>2315.300049</td>\n",
       "      <td>2315.300049</td>\n",
       "      <td>1433500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>2021-04-26</td>\n",
       "      <td>2319.929932</td>\n",
       "      <td>2341.260010</td>\n",
       "      <td>2313.840088</td>\n",
       "      <td>2326.739990</td>\n",
       "      <td>2326.739990</td>\n",
       "      <td>1041700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>2021-04-27</td>\n",
       "      <td>2336.000000</td>\n",
       "      <td>2337.449951</td>\n",
       "      <td>2304.270020</td>\n",
       "      <td>2307.120117</td>\n",
       "      <td>2307.120117</td>\n",
       "      <td>1587000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>504 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Date         Open         High          Low        Close  \\\n",
       "0    2019-04-29  1274.000000  1289.270020  1266.295044  1287.579956   \n",
       "1    2019-04-30  1185.000000  1192.810059  1175.000000  1188.479980   \n",
       "2    2019-05-01  1188.050049  1188.050049  1167.180054  1168.079956   \n",
       "3    2019-05-02  1167.760010  1174.189941  1155.001953  1162.609985   \n",
       "4    2019-05-03  1173.650024  1186.800049  1169.000000  1185.400024   \n",
       "..          ...          ...          ...          ...          ...   \n",
       "499  2021-04-21  2285.250000  2295.320068  2258.570068  2293.290039   \n",
       "500  2021-04-22  2293.229980  2303.761963  2256.449951  2267.919922   \n",
       "501  2021-04-23  2283.469971  2325.820068  2278.209961  2315.300049   \n",
       "502  2021-04-26  2319.929932  2341.260010  2313.840088  2326.739990   \n",
       "503  2021-04-27  2336.000000  2337.449951  2304.270020  2307.120117   \n",
       "\n",
       "       Adj Close   Volume  \n",
       "0    1287.579956  2499400  \n",
       "1    1188.479980  6207000  \n",
       "2    1168.079956  2639200  \n",
       "3    1162.609985  1944800  \n",
       "4    1185.400024  1980700  \n",
       "..           ...      ...  \n",
       "499  2293.290039  1196500  \n",
       "500  2267.919922  1054800  \n",
       "501  2315.300049  1433500  \n",
       "502  2326.739990  1041700  \n",
       "503  2307.120117  1587000  \n",
       "\n",
       "[504 rows x 7 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('google_stock_19-21.csv', sep=',', header=0, skiprows=0)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = [('Date','Date'),('Close','Close')]\n",
    "for i,row in df.iterrows():\n",
    "    for key in targets:\n",
    "        try:\n",
    "            master_dict[key[1]].append(row[key[0]])\n",
    "        except:\n",
    "            master_dict[key[1]] = [row[key[0]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Date': ['2019-04-29',\n",
       "  '2019-04-30',\n",
       "  '2019-05-01',\n",
       "  '2019-05-02',\n",
       "  '2019-05-03',\n",
       "  '2019-05-06',\n",
       "  '2019-05-07',\n",
       "  '2019-05-08',\n",
       "  '2019-05-09',\n",
       "  '2019-05-10',\n",
       "  '2019-05-13',\n",
       "  '2019-05-14',\n",
       "  '2019-05-15',\n",
       "  '2019-05-16',\n",
       "  '2019-05-17',\n",
       "  '2019-05-20',\n",
       "  '2019-05-21',\n",
       "  '2019-05-22',\n",
       "  '2019-05-23',\n",
       "  '2019-05-24',\n",
       "  '2019-05-28',\n",
       "  '2019-05-29',\n",
       "  '2019-05-30',\n",
       "  '2019-05-31',\n",
       "  '2019-06-03',\n",
       "  '2019-06-04',\n",
       "  '2019-06-05',\n",
       "  '2019-06-06',\n",
       "  '2019-06-07',\n",
       "  '2019-06-10',\n",
       "  '2019-06-11',\n",
       "  '2019-06-12',\n",
       "  '2019-06-13',\n",
       "  '2019-06-14',\n",
       "  '2019-06-17',\n",
       "  '2019-06-18',\n",
       "  '2019-06-19',\n",
       "  '2019-06-20',\n",
       "  '2019-06-21',\n",
       "  '2019-06-24',\n",
       "  '2019-06-25',\n",
       "  '2019-06-26',\n",
       "  '2019-06-27',\n",
       "  '2019-06-28',\n",
       "  '2019-07-01',\n",
       "  '2019-07-02',\n",
       "  '2019-07-03',\n",
       "  '2019-07-05',\n",
       "  '2019-07-08',\n",
       "  '2019-07-09',\n",
       "  '2019-07-10',\n",
       "  '2019-07-11',\n",
       "  '2019-07-12',\n",
       "  '2019-07-15',\n",
       "  '2019-07-16',\n",
       "  '2019-07-17',\n",
       "  '2019-07-18',\n",
       "  '2019-07-19',\n",
       "  '2019-07-22',\n",
       "  '2019-07-23',\n",
       "  '2019-07-24',\n",
       "  '2019-07-25',\n",
       "  '2019-07-26',\n",
       "  '2019-07-29',\n",
       "  '2019-07-30',\n",
       "  '2019-07-31',\n",
       "  '2019-08-01',\n",
       "  '2019-08-02',\n",
       "  '2019-08-05',\n",
       "  '2019-08-06',\n",
       "  '2019-08-07',\n",
       "  '2019-08-08',\n",
       "  '2019-08-09',\n",
       "  '2019-08-12',\n",
       "  '2019-08-13',\n",
       "  '2019-08-14',\n",
       "  '2019-08-15',\n",
       "  '2019-08-16',\n",
       "  '2019-08-19',\n",
       "  '2019-08-20',\n",
       "  '2019-08-21',\n",
       "  '2019-08-22',\n",
       "  '2019-08-23',\n",
       "  '2019-08-26',\n",
       "  '2019-08-27',\n",
       "  '2019-08-28',\n",
       "  '2019-08-29',\n",
       "  '2019-08-30',\n",
       "  '2019-09-03',\n",
       "  '2019-09-04',\n",
       "  '2019-09-05',\n",
       "  '2019-09-06',\n",
       "  '2019-09-09',\n",
       "  '2019-09-10',\n",
       "  '2019-09-11',\n",
       "  '2019-09-12',\n",
       "  '2019-09-13',\n",
       "  '2019-09-16',\n",
       "  '2019-09-17',\n",
       "  '2019-09-18',\n",
       "  '2019-09-19',\n",
       "  '2019-09-20',\n",
       "  '2019-09-23',\n",
       "  '2019-09-24',\n",
       "  '2019-09-25',\n",
       "  '2019-09-26',\n",
       "  '2019-09-27',\n",
       "  '2019-09-30',\n",
       "  '2019-10-01',\n",
       "  '2019-10-02',\n",
       "  '2019-10-03',\n",
       "  '2019-10-04',\n",
       "  '2019-10-07',\n",
       "  '2019-10-08',\n",
       "  '2019-10-09',\n",
       "  '2019-10-10',\n",
       "  '2019-10-11',\n",
       "  '2019-10-14',\n",
       "  '2019-10-15',\n",
       "  '2019-10-16',\n",
       "  '2019-10-17',\n",
       "  '2019-10-18',\n",
       "  '2019-10-21',\n",
       "  '2019-10-22',\n",
       "  '2019-10-23',\n",
       "  '2019-10-24',\n",
       "  '2019-10-25',\n",
       "  '2019-10-28',\n",
       "  '2019-10-29',\n",
       "  '2019-10-30',\n",
       "  '2019-10-31',\n",
       "  '2019-11-01',\n",
       "  '2019-11-04',\n",
       "  '2019-11-05',\n",
       "  '2019-11-06',\n",
       "  '2019-11-07',\n",
       "  '2019-11-08',\n",
       "  '2019-11-11',\n",
       "  '2019-11-12',\n",
       "  '2019-11-13',\n",
       "  '2019-11-14',\n",
       "  '2019-11-15',\n",
       "  '2019-11-18',\n",
       "  '2019-11-19',\n",
       "  '2019-11-20',\n",
       "  '2019-11-21',\n",
       "  '2019-11-22',\n",
       "  '2019-11-25',\n",
       "  '2019-11-26',\n",
       "  '2019-11-27',\n",
       "  '2019-11-29',\n",
       "  '2019-12-02',\n",
       "  '2019-12-03',\n",
       "  '2019-12-04',\n",
       "  '2019-12-05',\n",
       "  '2019-12-06',\n",
       "  '2019-12-09',\n",
       "  '2019-12-10',\n",
       "  '2019-12-11',\n",
       "  '2019-12-12',\n",
       "  '2019-12-13',\n",
       "  '2019-12-16',\n",
       "  '2019-12-17',\n",
       "  '2019-12-18',\n",
       "  '2019-12-19',\n",
       "  '2019-12-20',\n",
       "  '2019-12-23',\n",
       "  '2019-12-24',\n",
       "  '2019-12-26',\n",
       "  '2019-12-27',\n",
       "  '2019-12-30',\n",
       "  '2019-12-31',\n",
       "  '2020-01-02',\n",
       "  '2020-01-03',\n",
       "  '2020-01-06',\n",
       "  '2020-01-07',\n",
       "  '2020-01-08',\n",
       "  '2020-01-09',\n",
       "  '2020-01-10',\n",
       "  '2020-01-13',\n",
       "  '2020-01-14',\n",
       "  '2020-01-15',\n",
       "  '2020-01-16',\n",
       "  '2020-01-17',\n",
       "  '2020-01-21',\n",
       "  '2020-01-22',\n",
       "  '2020-01-23',\n",
       "  '2020-01-24',\n",
       "  '2020-01-27',\n",
       "  '2020-01-28',\n",
       "  '2020-01-29',\n",
       "  '2020-01-30',\n",
       "  '2020-01-31',\n",
       "  '2020-02-03',\n",
       "  '2020-02-04',\n",
       "  '2020-02-05',\n",
       "  '2020-02-06',\n",
       "  '2020-02-07',\n",
       "  '2020-02-10',\n",
       "  '2020-02-11',\n",
       "  '2020-02-12',\n",
       "  '2020-02-13',\n",
       "  '2020-02-14',\n",
       "  '2020-02-18',\n",
       "  '2020-02-19',\n",
       "  '2020-02-20',\n",
       "  '2020-02-21',\n",
       "  '2020-02-24',\n",
       "  '2020-02-25',\n",
       "  '2020-02-26',\n",
       "  '2020-02-27',\n",
       "  '2020-02-28',\n",
       "  '2020-03-02',\n",
       "  '2020-03-03',\n",
       "  '2020-03-04',\n",
       "  '2020-03-05',\n",
       "  '2020-03-06',\n",
       "  '2020-03-09',\n",
       "  '2020-03-10',\n",
       "  '2020-03-11',\n",
       "  '2020-03-12',\n",
       "  '2020-03-13',\n",
       "  '2020-03-16',\n",
       "  '2020-03-17',\n",
       "  '2020-03-18',\n",
       "  '2020-03-19',\n",
       "  '2020-03-20',\n",
       "  '2020-03-23',\n",
       "  '2020-03-24',\n",
       "  '2020-03-25',\n",
       "  '2020-03-26',\n",
       "  '2020-03-27',\n",
       "  '2020-03-30',\n",
       "  '2020-03-31',\n",
       "  '2020-04-01',\n",
       "  '2020-04-02',\n",
       "  '2020-04-03',\n",
       "  '2020-04-06',\n",
       "  '2020-04-07',\n",
       "  '2020-04-08',\n",
       "  '2020-04-09',\n",
       "  '2020-04-13',\n",
       "  '2020-04-14',\n",
       "  '2020-04-15',\n",
       "  '2020-04-16',\n",
       "  '2020-04-17',\n",
       "  '2020-04-20',\n",
       "  '2020-04-21',\n",
       "  '2020-04-22',\n",
       "  '2020-04-23',\n",
       "  '2020-04-24',\n",
       "  '2020-04-27',\n",
       "  '2020-04-28',\n",
       "  '2020-04-29',\n",
       "  '2020-04-30',\n",
       "  '2020-05-01',\n",
       "  '2020-05-04',\n",
       "  '2020-05-05',\n",
       "  '2020-05-06',\n",
       "  '2020-05-07',\n",
       "  '2020-05-08',\n",
       "  '2020-05-11',\n",
       "  '2020-05-12',\n",
       "  '2020-05-13',\n",
       "  '2020-05-14',\n",
       "  '2020-05-15',\n",
       "  '2020-05-18',\n",
       "  '2020-05-19',\n",
       "  '2020-05-20',\n",
       "  '2020-05-21',\n",
       "  '2020-05-22',\n",
       "  '2020-05-26',\n",
       "  '2020-05-27',\n",
       "  '2020-05-28',\n",
       "  '2020-05-29',\n",
       "  '2020-06-01',\n",
       "  '2020-06-02',\n",
       "  '2020-06-03',\n",
       "  '2020-06-04',\n",
       "  '2020-06-05',\n",
       "  '2020-06-08',\n",
       "  '2020-06-09',\n",
       "  '2020-06-10',\n",
       "  '2020-06-11',\n",
       "  '2020-06-12',\n",
       "  '2020-06-15',\n",
       "  '2020-06-16',\n",
       "  '2020-06-17',\n",
       "  '2020-06-18',\n",
       "  '2020-06-19',\n",
       "  '2020-06-22',\n",
       "  '2020-06-23',\n",
       "  '2020-06-24',\n",
       "  '2020-06-25',\n",
       "  '2020-06-26',\n",
       "  '2020-06-29',\n",
       "  '2020-06-30',\n",
       "  '2020-07-01',\n",
       "  '2020-07-02',\n",
       "  '2020-07-06',\n",
       "  '2020-07-07',\n",
       "  '2020-07-08',\n",
       "  '2020-07-09',\n",
       "  '2020-07-10',\n",
       "  '2020-07-13',\n",
       "  '2020-07-14',\n",
       "  '2020-07-15',\n",
       "  '2020-07-16',\n",
       "  '2020-07-17',\n",
       "  '2020-07-20',\n",
       "  '2020-07-21',\n",
       "  '2020-07-22',\n",
       "  '2020-07-23',\n",
       "  '2020-07-24',\n",
       "  '2020-07-27',\n",
       "  '2020-07-28',\n",
       "  '2020-07-29',\n",
       "  '2020-07-30',\n",
       "  '2020-07-31',\n",
       "  '2020-08-03',\n",
       "  '2020-08-04',\n",
       "  '2020-08-05',\n",
       "  '2020-08-06',\n",
       "  '2020-08-07',\n",
       "  '2020-08-10',\n",
       "  '2020-08-11',\n",
       "  '2020-08-12',\n",
       "  '2020-08-13',\n",
       "  '2020-08-14',\n",
       "  '2020-08-17',\n",
       "  '2020-08-18',\n",
       "  '2020-08-19',\n",
       "  '2020-08-20',\n",
       "  '2020-08-21',\n",
       "  '2020-08-24',\n",
       "  '2020-08-25',\n",
       "  '2020-08-26',\n",
       "  '2020-08-27',\n",
       "  '2020-08-28',\n",
       "  '2020-08-31',\n",
       "  '2020-09-01',\n",
       "  '2020-09-02',\n",
       "  '2020-09-03',\n",
       "  '2020-09-04',\n",
       "  '2020-09-08',\n",
       "  '2020-09-09',\n",
       "  '2020-09-10',\n",
       "  '2020-09-11',\n",
       "  '2020-09-14',\n",
       "  '2020-09-15',\n",
       "  '2020-09-16',\n",
       "  '2020-09-17',\n",
       "  '2020-09-18',\n",
       "  '2020-09-21',\n",
       "  '2020-09-22',\n",
       "  '2020-09-23',\n",
       "  '2020-09-24',\n",
       "  '2020-09-25',\n",
       "  '2020-09-28',\n",
       "  '2020-09-29',\n",
       "  '2020-09-30',\n",
       "  '2020-10-01',\n",
       "  '2020-10-02',\n",
       "  '2020-10-05',\n",
       "  '2020-10-06',\n",
       "  '2020-10-07',\n",
       "  '2020-10-08',\n",
       "  '2020-10-09',\n",
       "  '2020-10-12',\n",
       "  '2020-10-13',\n",
       "  '2020-10-14',\n",
       "  '2020-10-15',\n",
       "  '2020-10-16',\n",
       "  '2020-10-19',\n",
       "  '2020-10-20',\n",
       "  '2020-10-21',\n",
       "  '2020-10-22',\n",
       "  '2020-10-23',\n",
       "  '2020-10-26',\n",
       "  '2020-10-27',\n",
       "  '2020-10-28',\n",
       "  '2020-10-29',\n",
       "  '2020-10-30',\n",
       "  '2020-11-02',\n",
       "  '2020-11-03',\n",
       "  '2020-11-04',\n",
       "  '2020-11-05',\n",
       "  '2020-11-06',\n",
       "  '2020-11-09',\n",
       "  '2020-11-10',\n",
       "  '2020-11-11',\n",
       "  '2020-11-12',\n",
       "  '2020-11-13',\n",
       "  '2020-11-16',\n",
       "  '2020-11-17',\n",
       "  '2020-11-18',\n",
       "  '2020-11-19',\n",
       "  '2020-11-20',\n",
       "  '2020-11-23',\n",
       "  '2020-11-24',\n",
       "  '2020-11-25',\n",
       "  '2020-11-27',\n",
       "  '2020-11-30',\n",
       "  '2020-12-01',\n",
       "  '2020-12-02',\n",
       "  '2020-12-03',\n",
       "  '2020-12-04',\n",
       "  '2020-12-07',\n",
       "  '2020-12-08',\n",
       "  '2020-12-09',\n",
       "  '2020-12-10',\n",
       "  '2020-12-11',\n",
       "  '2020-12-14',\n",
       "  '2020-12-15',\n",
       "  '2020-12-16',\n",
       "  '2020-12-17',\n",
       "  '2020-12-18',\n",
       "  '2020-12-21',\n",
       "  '2020-12-22',\n",
       "  '2020-12-23',\n",
       "  '2020-12-24',\n",
       "  '2020-12-28',\n",
       "  '2020-12-29',\n",
       "  '2020-12-30',\n",
       "  '2020-12-31',\n",
       "  '2021-01-04',\n",
       "  '2021-01-05',\n",
       "  '2021-01-06',\n",
       "  '2021-01-07',\n",
       "  '2021-01-08',\n",
       "  '2021-01-11',\n",
       "  '2021-01-12',\n",
       "  '2021-01-13',\n",
       "  '2021-01-14',\n",
       "  '2021-01-15',\n",
       "  '2021-01-19',\n",
       "  '2021-01-20',\n",
       "  '2021-01-21',\n",
       "  '2021-01-22',\n",
       "  '2021-01-25',\n",
       "  '2021-01-26',\n",
       "  '2021-01-27',\n",
       "  '2021-01-28',\n",
       "  '2021-01-29',\n",
       "  '2021-02-01',\n",
       "  '2021-02-02',\n",
       "  '2021-02-03',\n",
       "  '2021-02-04',\n",
       "  '2021-02-05',\n",
       "  '2021-02-08',\n",
       "  '2021-02-09',\n",
       "  '2021-02-10',\n",
       "  '2021-02-11',\n",
       "  '2021-02-12',\n",
       "  '2021-02-16',\n",
       "  '2021-02-17',\n",
       "  '2021-02-18',\n",
       "  '2021-02-19',\n",
       "  '2021-02-22',\n",
       "  '2021-02-23',\n",
       "  '2021-02-24',\n",
       "  '2021-02-25',\n",
       "  '2021-02-26',\n",
       "  '2021-03-01',\n",
       "  '2021-03-02',\n",
       "  '2021-03-03',\n",
       "  '2021-03-04',\n",
       "  '2021-03-05',\n",
       "  '2021-03-08',\n",
       "  '2021-03-09',\n",
       "  '2021-03-10',\n",
       "  '2021-03-11',\n",
       "  '2021-03-12',\n",
       "  '2021-03-15',\n",
       "  '2021-03-16',\n",
       "  '2021-03-17',\n",
       "  '2021-03-18',\n",
       "  '2021-03-19',\n",
       "  '2021-03-22',\n",
       "  '2021-03-23',\n",
       "  '2021-03-24',\n",
       "  '2021-03-25',\n",
       "  '2021-03-26',\n",
       "  '2021-03-29',\n",
       "  '2021-03-30',\n",
       "  '2021-03-31',\n",
       "  '2021-04-01',\n",
       "  '2021-04-05',\n",
       "  '2021-04-06',\n",
       "  '2021-04-07',\n",
       "  '2021-04-08',\n",
       "  '2021-04-09',\n",
       "  '2021-04-12',\n",
       "  '2021-04-13',\n",
       "  '2021-04-14',\n",
       "  '2021-04-15',\n",
       "  '2021-04-16',\n",
       "  '2021-04-19',\n",
       "  '2021-04-20',\n",
       "  '2021-04-21',\n",
       "  '2021-04-22',\n",
       "  '2021-04-23',\n",
       "  '2021-04-26',\n",
       "  '2021-04-27'],\n",
       " 'Close': [1287.579956,\n",
       "  1188.47998,\n",
       "  1168.079956,\n",
       "  1162.609985,\n",
       "  1185.400024,\n",
       "  1189.390015,\n",
       "  1174.099976,\n",
       "  1166.27002,\n",
       "  1162.380005,\n",
       "  1164.27002,\n",
       "  1132.030029,\n",
       "  1120.439941,\n",
       "  1164.209961,\n",
       "  1178.97998,\n",
       "  1162.300049,\n",
       "  1138.849976,\n",
       "  1149.630005,\n",
       "  1151.420044,\n",
       "  1140.77002,\n",
       "  1133.469971,\n",
       "  1134.150024,\n",
       "  1116.459961,\n",
       "  1117.949951,\n",
       "  1103.630005,\n",
       "  1036.22998,\n",
       "  1053.050049,\n",
       "  1042.219971,\n",
       "  1044.339966,\n",
       "  1066.040039,\n",
       "  1080.380005,\n",
       "  1078.719971,\n",
       "  1077.030029,\n",
       "  1088.77002,\n",
       "  1085.349976,\n",
       "  1092.5,\n",
       "  1103.599976,\n",
       "  1102.329956,\n",
       "  1111.420044,\n",
       "  1121.880005,\n",
       "  1115.52002,\n",
       "  1086.349976,\n",
       "  1079.800049,\n",
       "  1076.01001,\n",
       "  1080.910034,\n",
       "  1097.949951,\n",
       "  1111.25,\n",
       "  1121.579956,\n",
       "  1131.589966,\n",
       "  1116.349976,\n",
       "  1124.829956,\n",
       "  1140.47998,\n",
       "  1144.209961,\n",
       "  1144.900024,\n",
       "  1150.339966,\n",
       "  1153.579956,\n",
       "  1146.349976,\n",
       "  1146.329956,\n",
       "  1130.099976,\n",
       "  1138.069946,\n",
       "  1146.209961,\n",
       "  1137.810059,\n",
       "  1132.119995,\n",
       "  1250.410034,\n",
       "  1239.410034,\n",
       "  1225.140015,\n",
       "  1216.680054,\n",
       "  1209.01001,\n",
       "  1193.98999,\n",
       "  1152.319946,\n",
       "  1169.949951,\n",
       "  1173.98999,\n",
       "  1204.800049,\n",
       "  1188.01001,\n",
       "  1174.709961,\n",
       "  1197.27002,\n",
       "  1164.290039,\n",
       "  1167.26001,\n",
       "  1177.599976,\n",
       "  1198.449951,\n",
       "  1182.689941,\n",
       "  1191.25,\n",
       "  1189.530029,\n",
       "  1151.290039,\n",
       "  1168.890015,\n",
       "  1167.839966,\n",
       "  1171.02002,\n",
       "  1192.849976,\n",
       "  1188.099976,\n",
       "  1168.390015,\n",
       "  1181.410034,\n",
       "  1211.380005,\n",
       "  1204.930054,\n",
       "  1204.410034,\n",
       "  1206.0,\n",
       "  1220.170044,\n",
       "  1234.25,\n",
       "  1239.560059,\n",
       "  1231.300049,\n",
       "  1229.150024,\n",
       "  1232.410034,\n",
       "  1238.709961,\n",
       "  1229.930054,\n",
       "  1234.030029,\n",
       "  1218.76001,\n",
       "  1246.52002,\n",
       "  1241.390015,\n",
       "  1225.089966,\n",
       "  1219.0,\n",
       "  1205.099976,\n",
       "  1176.630005,\n",
       "  1187.829956,\n",
       "  1209.0,\n",
       "  1207.680054,\n",
       "  1189.130005,\n",
       "  1202.310059,\n",
       "  1208.670044,\n",
       "  1215.449951,\n",
       "  1217.140015,\n",
       "  1243.01001,\n",
       "  1243.640015,\n",
       "  1253.069946,\n",
       "  1245.48999,\n",
       "  1246.150024,\n",
       "  1242.800049,\n",
       "  1259.130005,\n",
       "  1260.98999,\n",
       "  1265.130005,\n",
       "  1290.0,\n",
       "  1262.619995,\n",
       "  1261.290039,\n",
       "  1260.109985,\n",
       "  1273.73999,\n",
       "  1291.369995,\n",
       "  1292.030029,\n",
       "  1291.800049,\n",
       "  1308.859985,\n",
       "  1311.369995,\n",
       "  1299.189941,\n",
       "  1298.800049,\n",
       "  1298.0,\n",
       "  1311.459961,\n",
       "  1334.869995,\n",
       "  1320.699951,\n",
       "  1315.459961,\n",
       "  1303.050049,\n",
       "  1301.349976,\n",
       "  1295.339966,\n",
       "  1306.689941,\n",
       "  1313.550049,\n",
       "  1312.98999,\n",
       "  1304.959961,\n",
       "  1289.920044,\n",
       "  1295.280029,\n",
       "  1320.540039,\n",
       "  1328.130005,\n",
       "  1340.619995,\n",
       "  1343.560059,\n",
       "  1344.660034,\n",
       "  1345.02002,\n",
       "  1350.27002,\n",
       "  1347.829956,\n",
       "  1361.170044,\n",
       "  1355.119995,\n",
       "  1352.619995,\n",
       "  1356.040039,\n",
       "  1349.589966,\n",
       "  1348.839966,\n",
       "  1343.560059,\n",
       "  1360.400024,\n",
       "  1351.890015,\n",
       "  1336.140015,\n",
       "  1337.02002,\n",
       "  1367.369995,\n",
       "  1360.660034,\n",
       "  1394.209961,\n",
       "  1393.339966,\n",
       "  1404.319946,\n",
       "  1419.829956,\n",
       "  1429.72998,\n",
       "  1439.22998,\n",
       "  1430.880005,\n",
       "  1439.199951,\n",
       "  1451.699951,\n",
       "  1480.390015,\n",
       "  1484.400024,\n",
       "  1485.949951,\n",
       "  1486.650024,\n",
       "  1466.709961,\n",
       "  1433.900024,\n",
       "  1452.560059,\n",
       "  1458.630005,\n",
       "  1455.839966,\n",
       "  1434.22998,\n",
       "  1485.939941,\n",
       "  1447.069946,\n",
       "  1448.22998,\n",
       "  1476.22998,\n",
       "  1479.22998,\n",
       "  1508.680054,\n",
       "  1508.790039,\n",
       "  1518.27002,\n",
       "  1514.660034,\n",
       "  1520.73999,\n",
       "  1519.670044,\n",
       "  1526.689941,\n",
       "  1518.150024,\n",
       "  1485.109985,\n",
       "  1421.589966,\n",
       "  1388.449951,\n",
       "  1393.180054,\n",
       "  1318.089966,\n",
       "  1339.329956,\n",
       "  1389.109985,\n",
       "  1341.390015,\n",
       "  1386.52002,\n",
       "  1319.040039,\n",
       "  1298.410034,\n",
       "  1215.560059,\n",
       "  1280.390015,\n",
       "  1215.410034,\n",
       "  1114.910034,\n",
       "  1219.72998,\n",
       "  1084.329956,\n",
       "  1119.800049,\n",
       "  1096.800049,\n",
       "  1115.290039,\n",
       "  1072.319946,\n",
       "  1056.619995,\n",
       "  1134.459961,\n",
       "  1102.48999,\n",
       "  1161.75,\n",
       "  1110.709961,\n",
       "  1146.819946,\n",
       "  1162.810059,\n",
       "  1105.619995,\n",
       "  1120.839966,\n",
       "  1097.880005,\n",
       "  1186.920044,\n",
       "  1186.51001,\n",
       "  1210.280029,\n",
       "  1211.449951,\n",
       "  1217.560059,\n",
       "  1269.22998,\n",
       "  1262.469971,\n",
       "  1263.469971,\n",
       "  1283.25,\n",
       "  1266.609985,\n",
       "  1216.339966,\n",
       "  1263.209961,\n",
       "  1276.310059,\n",
       "  1279.310059,\n",
       "  1275.880005,\n",
       "  1233.670044,\n",
       "  1341.47998,\n",
       "  1348.660034,\n",
       "  1320.609985,\n",
       "  1326.800049,\n",
       "  1351.109985,\n",
       "  1347.300049,\n",
       "  1372.560059,\n",
       "  1388.369995,\n",
       "  1403.26001,\n",
       "  1375.73999,\n",
       "  1349.329956,\n",
       "  1356.130005,\n",
       "  1373.189941,\n",
       "  1383.939941,\n",
       "  1373.484985,\n",
       "  1406.719971,\n",
       "  1402.800049,\n",
       "  1410.420044,\n",
       "  1417.02002,\n",
       "  1417.839966,\n",
       "  1416.72998,\n",
       "  1428.920044,\n",
       "  1431.819946,\n",
       "  1439.219971,\n",
       "  1436.380005,\n",
       "  1412.180054,\n",
       "  1438.390015,\n",
       "  1446.609985,\n",
       "  1456.160034,\n",
       "  1465.849976,\n",
       "  1403.839966,\n",
       "  1413.180054,\n",
       "  1419.849976,\n",
       "  1442.719971,\n",
       "  1451.119995,\n",
       "  1435.959961,\n",
       "  1431.719971,\n",
       "  1451.859985,\n",
       "  1464.410034,\n",
       "  1431.969971,\n",
       "  1441.329956,\n",
       "  1359.900024,\n",
       "  1394.969971,\n",
       "  1413.609985,\n",
       "  1438.040039,\n",
       "  1464.699951,\n",
       "  1495.699951,\n",
       "  1485.180054,\n",
       "  1496.0,\n",
       "  1510.98999,\n",
       "  1541.73999,\n",
       "  1511.339966,\n",
       "  1520.579956,\n",
       "  1513.640015,\n",
       "  1518.0,\n",
       "  1515.550049,\n",
       "  1565.719971,\n",
       "  1558.420044,\n",
       "  1568.48999,\n",
       "  1515.680054,\n",
       "  1511.869995,\n",
       "  1530.199951,\n",
       "  1500.339966,\n",
       "  1522.02002,\n",
       "  1531.449951,\n",
       "  1482.959961,\n",
       "  1474.449951,\n",
       "  1464.969971,\n",
       "  1473.609985,\n",
       "  1500.099976,\n",
       "  1494.48999,\n",
       "  1496.099976,\n",
       "  1480.319946,\n",
       "  1506.619995,\n",
       "  1518.449951,\n",
       "  1507.72998,\n",
       "  1517.97998,\n",
       "  1558.599976,\n",
       "  1547.530029,\n",
       "  1581.75,\n",
       "  1580.420044,\n",
       "  1588.199951,\n",
       "  1608.219971,\n",
       "  1652.380005,\n",
       "  1634.329956,\n",
       "  1644.410034,\n",
       "  1634.180054,\n",
       "  1660.709961,\n",
       "  1728.280029,\n",
       "  1641.839966,\n",
       "  1591.040039,\n",
       "  1532.390015,\n",
       "  1556.959961,\n",
       "  1532.02002,\n",
       "  1520.719971,\n",
       "  1519.280029,\n",
       "  1541.439941,\n",
       "  1520.900024,\n",
       "  1495.530029,\n",
       "  1459.98999,\n",
       "  1431.160034,\n",
       "  1465.459961,\n",
       "  1415.209961,\n",
       "  1428.290039,\n",
       "  1444.959961,\n",
       "  1464.52002,\n",
       "  1469.329956,\n",
       "  1469.599976,\n",
       "  1490.089966,\n",
       "  1458.420044,\n",
       "  1486.02002,\n",
       "  1453.439941,\n",
       "  1460.290039,\n",
       "  1485.930054,\n",
       "  1515.219971,\n",
       "  1569.150024,\n",
       "  1571.680054,\n",
       "  1568.079956,\n",
       "  1559.130005,\n",
       "  1573.01001,\n",
       "  1534.609985,\n",
       "  1555.930054,\n",
       "  1593.310059,\n",
       "  1615.329956,\n",
       "  1641.0,\n",
       "  1590.449951,\n",
       "  1604.26001,\n",
       "  1516.619995,\n",
       "  1567.23999,\n",
       "  1621.01001,\n",
       "  1626.030029,\n",
       "  1650.209961,\n",
       "  1749.130005,\n",
       "  1763.369995,\n",
       "  1761.75,\n",
       "  1763.0,\n",
       "  1740.390015,\n",
       "  1752.709961,\n",
       "  1749.839966,\n",
       "  1777.02002,\n",
       "  1781.380005,\n",
       "  1770.150024,\n",
       "  1746.780029,\n",
       "  1763.920044,\n",
       "  1742.189941,\n",
       "  1734.859985,\n",
       "  1768.880005,\n",
       "  1771.430054,\n",
       "  1793.189941,\n",
       "  1760.73999,\n",
       "  1798.099976,\n",
       "  1827.949951,\n",
       "  1826.77002,\n",
       "  1827.98999,\n",
       "  1819.47998,\n",
       "  1818.550049,\n",
       "  1784.130005,\n",
       "  1775.329956,\n",
       "  1781.77002,\n",
       "  1760.060059,\n",
       "  1767.77002,\n",
       "  1763.0,\n",
       "  1747.900024,\n",
       "  1731.01001,\n",
       "  1739.369995,\n",
       "  1723.5,\n",
       "  1732.380005,\n",
       "  1738.849976,\n",
       "  1776.089966,\n",
       "  1758.719971,\n",
       "  1739.52002,\n",
       "  1751.880005,\n",
       "  1728.23999,\n",
       "  1740.920044,\n",
       "  1735.290039,\n",
       "  1787.25,\n",
       "  1807.209961,\n",
       "  1766.719971,\n",
       "  1746.550049,\n",
       "  1754.400024,\n",
       "  1740.180054,\n",
       "  1736.189941,\n",
       "  1790.859985,\n",
       "  1886.900024,\n",
       "  1891.25,\n",
       "  1901.050049,\n",
       "  1899.400024,\n",
       "  1917.23999,\n",
       "  1830.790039,\n",
       "  1863.109985,\n",
       "  1835.73999,\n",
       "  1901.349976,\n",
       "  1927.51001,\n",
       "  2070.070068,\n",
       "  2062.370117,\n",
       "  2098.0,\n",
       "  2092.909912,\n",
       "  2083.51001,\n",
       "  2095.379883,\n",
       "  2095.889893,\n",
       "  2104.110107,\n",
       "  2121.899902,\n",
       "  2128.310059,\n",
       "  2117.199951,\n",
       "  2101.139893,\n",
       "  2064.879883,\n",
       "  2070.860107,\n",
       "  2095.169922,\n",
       "  2031.359985,\n",
       "  2036.859985,\n",
       "  2081.51001,\n",
       "  2075.840088,\n",
       "  2026.709961,\n",
       "  2049.090088,\n",
       "  2108.540039,\n",
       "  2024.170044,\n",
       "  2052.699951,\n",
       "  2055.030029,\n",
       "  2114.77002,\n",
       "  2061.919922,\n",
       "  2066.48999,\n",
       "  2092.52002,\n",
       "  2091.080078,\n",
       "  2036.219971,\n",
       "  2043.199951,\n",
       "  2038.589966,\n",
       "  2052.959961,\n",
       "  2045.060059,\n",
       "  2044.359985,\n",
       "  2035.550049,\n",
       "  2055.949951,\n",
       "  2055.540039,\n",
       "  2068.629883,\n",
       "  2137.75,\n",
       "  2225.550049,\n",
       "  2224.75,\n",
       "  2249.679932,\n",
       "  2265.439941,\n",
       "  2285.879883,\n",
       "  2254.790039,\n",
       "  2267.27002,\n",
       "  2254.840088,\n",
       "  2296.659912,\n",
       "  2297.76001,\n",
       "  2302.399902,\n",
       "  2293.629883,\n",
       "  2293.290039,\n",
       "  2267.919922,\n",
       "  2315.300049,\n",
       "  2326.73999,\n",
       "  2307.120117]}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "master_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('data/stock_05-17.pkl', 'wb') as file:\n",
    "    pickle.dump(master_dict, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('data/stock_19-21.pkl', 'wb') as file:\n",
    "    pickle.dump(master_dict, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/stock_05-17.pkl', 'rb') as file:\n",
    "    train_dict = pickle.load(file)\n",
    "with open('data/stock_19-21.pkl', 'rb') as file:\n",
    "    test_dict = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make tuples\n",
    "train_data = []\n",
    "for (date, close) in zip(train_dict['Date'], train_dict['Close']):\n",
    "    train_data.append((date, close))\n",
    "    \n",
    "test_data = []\n",
    "for (date, close) in zip(test_dict['Date'], test_dict['Close']):\n",
    "    test_data.append((date, close))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jschu\\miniconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\loss.py:446: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1000], Step [100/100], Loss: 0.0028\n",
      "Epoch [2/1000], Step [100/100], Loss: 0.0026\n",
      "Epoch [3/1000], Step [100/100], Loss: 0.0050\n",
      "Epoch [4/1000], Step [100/100], Loss: 0.0130\n",
      "Epoch [5/1000], Step [100/100], Loss: 0.0002\n",
      "Epoch [6/1000], Step [100/100], Loss: 0.0034\n",
      "Epoch [7/1000], Step [100/100], Loss: 0.0000\n",
      "Epoch [8/1000], Step [100/100], Loss: 0.0002\n",
      "Epoch [9/1000], Step [100/100], Loss: 0.0004\n",
      "Epoch [10/1000], Step [100/100], Loss: 0.0004\n",
      "Epoch [11/1000], Step [100/100], Loss: 0.0002\n",
      "Epoch [12/1000], Step [100/100], Loss: 0.0003\n",
      "Epoch [13/1000], Step [100/100], Loss: 0.0001\n",
      "Epoch [14/1000], Step [100/100], Loss: 0.0001\n",
      "Epoch [15/1000], Step [100/100], Loss: 0.0001\n",
      "Epoch [16/1000], Step [100/100], Loss: 0.0001\n",
      "Epoch [17/1000], Step [100/100], Loss: 0.0003\n",
      "Epoch [18/1000], Step [100/100], Loss: 0.0001\n",
      "Epoch [19/1000], Step [100/100], Loss: 0.0000\n",
      "Epoch [20/1000], Step [100/100], Loss: 0.0001\n",
      "Epoch [21/1000], Step [100/100], Loss: 0.0000\n",
      "Epoch [22/1000], Step [100/100], Loss: 0.0003\n",
      "Epoch [23/1000], Step [100/100], Loss: 0.0003\n",
      "Epoch [24/1000], Step [100/100], Loss: 0.0000\n",
      "Epoch [25/1000], Step [100/100], Loss: 0.0000\n",
      "Epoch [26/1000], Step [100/100], Loss: 0.0006\n",
      "Epoch [27/1000], Step [100/100], Loss: 0.0000\n",
      "Epoch [28/1000], Step [100/100], Loss: 0.0003\n",
      "Epoch [29/1000], Step [100/100], Loss: 0.0005\n",
      "Epoch [30/1000], Step [100/100], Loss: 0.0000\n",
      "Epoch [31/1000], Step [100/100], Loss: 0.0000\n",
      "Epoch [32/1000], Step [100/100], Loss: 0.0001\n",
      "Epoch [33/1000], Step [100/100], Loss: 0.0000\n",
      "Epoch [34/1000], Step [100/100], Loss: 0.0001\n",
      "Epoch [35/1000], Step [100/100], Loss: 0.0000\n",
      "Epoch [36/1000], Step [100/100], Loss: 0.0000\n",
      "Epoch [37/1000], Step [100/100], Loss: 0.0000\n",
      "Epoch [38/1000], Step [100/100], Loss: 0.0000\n",
      "Epoch [39/1000], Step [100/100], Loss: 0.0001\n",
      "Epoch [40/1000], Step [100/100], Loss: 0.0000\n",
      "Epoch [41/1000], Step [100/100], Loss: 0.0000\n",
      "Epoch [42/1000], Step [100/100], Loss: 0.0001\n",
      "Epoch [43/1000], Step [100/100], Loss: 0.0005\n",
      "Epoch [44/1000], Step [100/100], Loss: 0.0000\n",
      "Epoch [45/1000], Step [100/100], Loss: 0.0000\n",
      "Epoch [46/1000], Step [100/100], Loss: 0.0003\n",
      "Epoch [47/1000], Step [100/100], Loss: 0.0000\n",
      "Epoch [48/1000], Step [100/100], Loss: 0.0000\n",
      "Epoch [49/1000], Step [100/100], Loss: 0.0000\n",
      "Epoch [50/1000], Step [100/100], Loss: 0.0000\n",
      "Epoch [51/1000], Step [100/100], Loss: 0.0001\n",
      "Epoch [52/1000], Step [100/100], Loss: 0.0001\n",
      "Epoch [53/1000], Step [100/100], Loss: 0.0000\n",
      "Epoch [54/1000], Step [100/100], Loss: 0.0007\n",
      "Epoch [55/1000], Step [100/100], Loss: 0.0002\n",
      "Epoch [56/1000], Step [100/100], Loss: 0.0003\n",
      "Epoch [57/1000], Step [100/100], Loss: 0.0005\n",
      "Epoch [58/1000], Step [100/100], Loss: 0.0001\n",
      "Epoch [59/1000], Step [100/100], Loss: 0.0003\n",
      "Epoch [60/1000], Step [100/100], Loss: 0.0002\n",
      "Epoch [61/1000], Step [100/100], Loss: 0.0005\n",
      "Epoch [62/1000], Step [100/100], Loss: 0.0001\n",
      "Epoch [63/1000], Step [100/100], Loss: 0.0000\n",
      "Epoch [64/1000], Step [100/100], Loss: 0.0000\n",
      "Epoch [65/1000], Step [100/100], Loss: 0.0000\n",
      "Epoch [66/1000], Step [100/100], Loss: 0.0000\n",
      "Epoch [67/1000], Step [100/100], Loss: 0.0001\n",
      "Epoch [68/1000], Step [100/100], Loss: 0.0000\n",
      "Epoch [69/1000], Step [100/100], Loss: 0.0000\n",
      "Epoch [70/1000], Step [100/100], Loss: 0.0000\n",
      "Epoch [71/1000], Step [100/100], Loss: 0.0000\n",
      "Epoch [72/1000], Step [100/100], Loss: 0.0001\n",
      "Epoch [73/1000], Step [100/100], Loss: 0.0000\n",
      "Epoch [74/1000], Step [100/100], Loss: 0.0000\n",
      "Epoch [75/1000], Step [100/100], Loss: 0.0000\n",
      "Epoch [76/1000], Step [100/100], Loss: 0.0000\n",
      "Epoch [77/1000], Step [100/100], Loss: 0.0000\n",
      "Epoch [78/1000], Step [100/100], Loss: 0.0000\n",
      "Epoch [79/1000], Step [100/100], Loss: 0.0001\n",
      "Epoch [80/1000], Step [100/100], Loss: 0.0000\n",
      "Epoch [81/1000], Step [100/100], Loss: 0.0001\n",
      "Epoch [82/1000], Step [100/100], Loss: 0.0001\n",
      "Epoch [83/1000], Step [100/100], Loss: 0.0000\n",
      "Epoch [84/1000], Step [100/100], Loss: 0.0000\n",
      "Epoch [85/1000], Step [100/100], Loss: 0.0001\n",
      "Epoch [86/1000], Step [100/100], Loss: 0.0001\n",
      "Epoch [87/1000], Step [100/100], Loss: 0.0000\n",
      "Epoch [88/1000], Step [100/100], Loss: 0.0001\n",
      "Epoch [89/1000], Step [100/100], Loss: 0.0001\n",
      "Epoch [90/1000], Step [100/100], Loss: 0.0001\n",
      "Epoch [91/1000], Step [100/100], Loss: 0.0000\n",
      "Epoch [92/1000], Step [100/100], Loss: 0.0000\n",
      "Epoch [93/1000], Step [100/100], Loss: 0.0000\n",
      "Epoch [94/1000], Step [100/100], Loss: 0.0000\n",
      "Epoch [95/1000], Step [100/100], Loss: 0.0000\n",
      "Epoch [96/1000], Step [100/100], Loss: 0.0000\n",
      "Epoch [97/1000], Step [100/100], Loss: 0.0001\n",
      "Epoch [98/1000], Step [100/100], Loss: 0.0000\n",
      "Epoch [99/1000], Step [100/100], Loss: 0.0000\n",
      "Epoch [100/1000], Step [100/100], Loss: 0.0000\n",
      "Epoch [101/1000], Step [100/100], Loss: 0.0001\n",
      "Epoch [102/1000], Step [100/100], Loss: 0.0001\n",
      "Epoch [103/1000], Step [100/100], Loss: 0.0000\n",
      "Epoch [104/1000], Step [100/100], Loss: 0.0000\n",
      "Epoch [105/1000], Step [100/100], Loss: 0.0001\n",
      "Epoch [106/1000], Step [100/100], Loss: 0.0000\n",
      "Epoch [107/1000], Step [100/100], Loss: 0.0000\n",
      "Epoch [108/1000], Step [100/100], Loss: 0.0001\n",
      "Epoch [109/1000], Step [100/100], Loss: 0.0001\n",
      "Epoch [110/1000], Step [100/100], Loss: 0.0000\n",
      "Epoch [111/1000], Step [100/100], Loss: 0.0001\n",
      "Epoch [112/1000], Step [100/100], Loss: 0.0000\n",
      "Epoch [113/1000], Step [100/100], Loss: 0.0000\n",
      "Epoch [114/1000], Step [100/100], Loss: 0.0000\n",
      "Epoch [115/1000], Step [100/100], Loss: 0.0000\n",
      "Epoch [116/1000], Step [100/100], Loss: 0.0000\n",
      "Epoch [117/1000], Step [100/100], Loss: 0.0000\n",
      "Epoch [118/1000], Step [100/100], Loss: 0.0000\n",
      "Epoch [119/1000], Step [100/100], Loss: 0.0001\n",
      "Epoch [120/1000], Step [100/100], Loss: 0.0001\n",
      "Epoch [121/1000], Step [100/100], Loss: 0.0000\n",
      "Epoch [122/1000], Step [100/100], Loss: 0.0015\n",
      "Epoch [123/1000], Step [100/100], Loss: 0.0001\n",
      "Epoch [124/1000], Step [100/100], Loss: 0.0000\n",
      "Epoch [125/1000], Step [100/100], Loss: 0.0000\n",
      "Epoch [126/1000], Step [100/100], Loss: 0.0003\n",
      "Epoch [127/1000], Step [100/100], Loss: 0.0000\n",
      "Epoch [128/1000], Step [100/100], Loss: 0.0000\n",
      "Epoch [129/1000], Step [100/100], Loss: 0.0000\n",
      "Epoch [130/1000], Step [100/100], Loss: 0.0000\n",
      "Epoch [131/1000], Step [100/100], Loss: 0.0000\n",
      "Epoch [132/1000], Step [100/100], Loss: 0.0000\n",
      "Epoch [133/1000], Step [100/100], Loss: 0.0002\n",
      "Epoch [134/1000], Step [100/100], Loss: 0.0000\n",
      "Epoch [135/1000], Step [100/100], Loss: 0.0000\n",
      "Epoch [136/1000], Step [100/100], Loss: 0.0000\n",
      "Epoch [137/1000], Step [100/100], Loss: 0.0000\n",
      "Epoch [138/1000], Step [100/100], Loss: 0.0000\n",
      "Epoch [139/1000], Step [100/100], Loss: 0.0000\n",
      "Epoch [140/1000], Step [100/100], Loss: 0.0000\n",
      "Epoch [141/1000], Step [100/100], Loss: 0.0000\n",
      "Epoch [142/1000], Step [100/100], Loss: 0.0011\n",
      "Epoch [143/1000], Step [100/100], Loss: 0.0000\n",
      "Epoch [144/1000], Step [100/100], Loss: 0.0000\n",
      "Epoch [145/1000], Step [100/100], Loss: 0.0000\n",
      "Epoch [146/1000], Step [100/100], Loss: 0.0000\n",
      "Epoch [147/1000], Step [100/100], Loss: 0.0000\n",
      "Epoch [148/1000], Step [100/100], Loss: 0.0001\n",
      "Epoch [149/1000], Step [100/100], Loss: 0.0001\n",
      "Epoch [150/1000], Step [100/100], Loss: 0.0000\n",
      "Epoch [151/1000], Step [100/100], Loss: 0.0001\n",
      "Epoch [152/1000], Step [100/100], Loss: 0.0000\n",
      "Epoch [153/1000], Step [100/100], Loss: 0.0000\n",
      "Epoch [154/1000], Step [100/100], Loss: 0.0000\n",
      "Epoch [155/1000], Step [100/100], Loss: 0.0001\n",
      "Epoch [156/1000], Step [100/100], Loss: 0.0000\n",
      "Epoch [157/1000], Step [100/100], Loss: 0.0001\n",
      "Epoch [158/1000], Step [100/100], Loss: 0.0000\n",
      "Epoch [159/1000], Step [100/100], Loss: 0.0000\n",
      "Epoch [160/1000], Step [100/100], Loss: 0.0002\n",
      "Epoch [161/1000], Step [100/100], Loss: 0.0000\n",
      "Epoch [162/1000], Step [100/100], Loss: 0.0000\n",
      "Epoch [163/1000], Step [100/100], Loss: 0.0000\n",
      "Epoch [164/1000], Step [100/100], Loss: 0.0000\n",
      "Epoch [165/1000], Step [100/100], Loss: 0.0000\n",
      "Epoch [166/1000], Step [100/100], Loss: 0.0000\n",
      "Epoch [167/1000], Step [100/100], Loss: 0.0000\n",
      "Epoch [168/1000], Step [100/100], Loss: 0.0000\n",
      "Epoch [169/1000], Step [100/100], Loss: 0.0000\n",
      "Epoch [170/1000], Step [100/100], Loss: 0.0000\n",
      "Epoch [171/1000], Step [100/100], Loss: 0.0000\n",
      "Epoch [172/1000], Step [100/100], Loss: 0.0000\n",
      "Epoch [173/1000], Step [100/100], Loss: 0.0000\n",
      "Epoch [174/1000], Step [100/100], Loss: 0.0000\n",
      "Epoch [175/1000], Step [100/100], Loss: 0.0004\n",
      "Epoch [176/1000], Step [100/100], Loss: 0.0000\n",
      "Epoch [177/1000], Step [100/100], Loss: 0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [178/1000], Step [100/100], Loss: 0.0000\n",
      "Epoch [179/1000], Step [100/100], Loss: 0.0001\n",
      "Epoch [180/1000], Step [100/100], Loss: 0.0000\n",
      "Epoch [181/1000], Step [100/100], Loss: 0.0000\n",
      "Epoch [182/1000], Step [100/100], Loss: 0.0000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-213-e5c94450c626>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\jschu\\miniconda3\\envs\\torch\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[1;32m--> 221\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    222\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\jschu\\miniconda3\\envs\\torch\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m    130\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 132\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m    133\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import random\n",
    "epochs = 1000\n",
    "batch_size = 100\n",
    "learning_rate = .0001\n",
    "\n",
    "net = myLSTM(1, 200, 1, 1)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "optimizer.zero_grad()\n",
    "\n",
    "\n",
    "#train\n",
    "running_loss = []\n",
    "for e in range(epochs):\n",
    "    for i in range(batch_size):\n",
    "        ind = random.randint(0,len(train_data)-80)\n",
    "        input = torch.FloatTensor(np.array([[price / 2500.0] for (date, price) in train_data[ind:ind+64]]) ).unsqueeze(0)\n",
    "        output = net(input)\n",
    "        \n",
    "        label = torch.FloatTensor([np.max([price for (date, price) in train_data[ind+64:ind+71]]) / 2500.0])\n",
    "\n",
    "        # Backward and optimize\n",
    "#         print(input, output, label)\n",
    "        loss = criterion(output, label)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                   .format(e+1, epochs, i+1, batch_size, loss.item()))\n",
    "        running_loss.append(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "plt.plot(running_loss)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# double check data\n",
    "ind = 300\n",
    "test_set = [[price / 2500.0] for (date, price) in train_data[ind:ind+71]]\n",
    "train_set = [[price / 2500.0] for (date, price) in train_data[ind:ind+64]]\n",
    "label = np.max([price for (date, price) in train_data[ind+64:ind+71]]) / 2500.0\n",
    "\n",
    "plt.plot(test_set)\n",
    "plt.plot(train_set)\n",
    "\n",
    "# last seven\n",
    "# plt.axline((64,1200),(64,1201), c='green')\n",
    "# predicted max\n",
    "plt.axline((0,label),(1,label), c='red')\n",
    "# actual max\n",
    "# plt.axline((0,max_test),(1,max_test), c='orange')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[209.86, 210.53, 211.09, 213.86, 213.93, 213.46, 213.54]"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[price for (date, price) in train_data[ind+64:ind+71]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build test set\n",
    "new_test_set = []\n",
    "test_size = 100\n",
    "for i in range(test_size):\n",
    "    ind = random.randint(0,len(test_data)-80)\n",
    "    new_test_set.append( (\n",
    "        torch.Tensor(np.array([[price] for (date, price) in test_data[ind:ind+64]]) ).unsqueeze(0),\n",
    "        torch.Tensor([np.max([price for (date, price) in test_data[ind+64:ind+71]]) ])) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1532.69765625\n"
     ]
    }
   ],
   "source": [
    "net.eval()\n",
    "error = []\n",
    "with torch.no_grad():\n",
    "    for (test, label) in new_test_set:\n",
    "        output = net(test)\n",
    "\n",
    "        error.append(np.abs(output.detach().numpy()[0][0] - label.detach().numpy()[0]))\n",
    "\n",
    "    print(np.sum(error) / len(error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'float' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-161-ecf9b3b1f1b2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mgraph_set\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mprice\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mprice\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgraph_set\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'float' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "graph_set[0][0].detach().numpy()[0]\n",
    "[price[0] for price in graph_set[0][0].detach().numpy()[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "386"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind = random.randint(0,len(test_data)-80)\n",
    "ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1225.140015 1127.4635046720505\n",
      "1253.069946 1155.6192487478256\n",
      "1360.400024 1303.672045469284\n",
      "1386.52002 1315.7042860984802\n",
      "1410.420044 1325.712651014328\n",
      "1531.449951 1483.78387093544\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "ind = [0, 50, 100, 150, 200, 250]\n",
    "\n",
    "net.eval()\n",
    "with torch.no_grad():\n",
    "    for i in ind:\n",
    "        graph_set = [price for (date, price) in test_data[i:i+71]]\n",
    "        test_graph = [price for (date, price) in test_data[i:i+64]]\n",
    "        test_graph_label = [price for (date, price) in test_data[i+64:i+71]]\n",
    "\n",
    "        max_test = np.max(test_graph_label)\n",
    "        output = net(torch.Tensor(np.array([[price/2500] for (date, price) in test_data[i:i+64]]) ).unsqueeze(0))\n",
    "\n",
    "        output = output.detach().numpy()[0][0] * 2500\n",
    "        print(max_test, output)\n",
    "        # # next seven\n",
    "        # for price in test_data[ind+64:ind+71]:\n",
    "        #     graph_set.append(price[1])\n",
    "\n",
    "        plt.plot(graph_set)\n",
    "        plt.plot(test_graph)\n",
    "        # last seven\n",
    "    #     plt.axline((64,.5),(64,.6), c='green')\n",
    "        # predicted max\n",
    "        plt.axline((0,output),(1,output), c='red')\n",
    "        # actual max\n",
    "        plt.axline((0,max_test),(1,max_test), c='orange')\n",
    "\n",
    "        plt.savefig('output/'+str(i)+'_stockspred.jpg')\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# max and min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [2/500], Step [100/100], Loss: 0.0038\n",
      "Epoch [3/500], Step [100/100], Loss: 0.0060\n",
      "Epoch [4/500], Step [100/100], Loss: 0.0001\n",
      "Epoch [5/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [6/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [7/500], Step [100/100], Loss: 0.0012\n",
      "Epoch [8/500], Step [100/100], Loss: 0.0002\n",
      "Epoch [9/500], Step [100/100], Loss: 0.0006\n",
      "Epoch [10/500], Step [100/100], Loss: 0.0001\n",
      "Epoch [11/500], Step [100/100], Loss: 0.0002\n",
      "Epoch [12/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [13/500], Step [100/100], Loss: 0.0001\n",
      "Epoch [14/500], Step [100/100], Loss: 0.0001\n",
      "Epoch [15/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [16/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [17/500], Step [100/100], Loss: 0.0001\n",
      "Epoch [18/500], Step [100/100], Loss: 0.0003\n",
      "Epoch [19/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [20/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [21/500], Step [100/100], Loss: 0.0001\n",
      "Epoch [22/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [23/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [24/500], Step [100/100], Loss: 0.0002\n",
      "Epoch [25/500], Step [100/100], Loss: 0.0001\n",
      "Epoch [26/500], Step [100/100], Loss: 0.0001\n",
      "Epoch [27/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [28/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [29/500], Step [100/100], Loss: 0.0001\n",
      "Epoch [30/500], Step [100/100], Loss: 0.0002\n",
      "Epoch [31/500], Step [100/100], Loss: 0.0003\n",
      "Epoch [32/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [33/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [34/500], Step [100/100], Loss: 0.0002\n",
      "Epoch [35/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [36/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [37/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [38/500], Step [100/100], Loss: 0.0001\n",
      "Epoch [39/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [40/500], Step [100/100], Loss: 0.0001\n",
      "Epoch [41/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [42/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [43/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [44/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [45/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [46/500], Step [100/100], Loss: 0.0001\n",
      "Epoch [47/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [48/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [49/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [50/500], Step [100/100], Loss: 0.0001\n",
      "Epoch [51/500], Step [100/100], Loss: 0.0002\n",
      "Epoch [52/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [53/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [54/500], Step [100/100], Loss: 0.0001\n",
      "Epoch [55/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [56/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [57/500], Step [100/100], Loss: 0.0001\n",
      "Epoch [58/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [59/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [60/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [61/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [62/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [63/500], Step [100/100], Loss: 0.0001\n",
      "Epoch [64/500], Step [100/100], Loss: 0.0001\n",
      "Epoch [65/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [66/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [67/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [68/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [69/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [70/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [71/500], Step [100/100], Loss: 0.0001\n",
      "Epoch [72/500], Step [100/100], Loss: 0.0001\n",
      "Epoch [73/500], Step [100/100], Loss: 0.0005\n",
      "Epoch [74/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [75/500], Step [100/100], Loss: 0.0001\n",
      "Epoch [76/500], Step [100/100], Loss: 0.0001\n",
      "Epoch [77/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [78/500], Step [100/100], Loss: 0.0001\n",
      "Epoch [79/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [80/500], Step [100/100], Loss: 0.0004\n",
      "Epoch [81/500], Step [100/100], Loss: 0.0001\n",
      "Epoch [82/500], Step [100/100], Loss: 0.0002\n",
      "Epoch [83/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [84/500], Step [100/100], Loss: 0.0001\n",
      "Epoch [85/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [86/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [87/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [88/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [89/500], Step [100/100], Loss: 0.0001\n",
      "Epoch [90/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [91/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [92/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [93/500], Step [100/100], Loss: 0.0003\n",
      "Epoch [94/500], Step [100/100], Loss: 0.0004\n",
      "Epoch [95/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [96/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [97/500], Step [100/100], Loss: 0.0001\n",
      "Epoch [98/500], Step [100/100], Loss: 0.0001\n",
      "Epoch [99/500], Step [100/100], Loss: 0.0004\n",
      "Epoch [100/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [101/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [102/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [103/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [104/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [105/500], Step [100/100], Loss: 0.0001\n",
      "Epoch [106/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [107/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [108/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [109/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [110/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [111/500], Step [100/100], Loss: 0.0002\n",
      "Epoch [112/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [113/500], Step [100/100], Loss: 0.0001\n",
      "Epoch [114/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [115/500], Step [100/100], Loss: 0.0001\n",
      "Epoch [116/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [117/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [118/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [119/500], Step [100/100], Loss: 0.0001\n",
      "Epoch [120/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [121/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [122/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [123/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [124/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [125/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [126/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [127/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [128/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [129/500], Step [100/100], Loss: 0.0001\n",
      "Epoch [130/500], Step [100/100], Loss: 0.0002\n",
      "Epoch [131/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [132/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [133/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [134/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [135/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [136/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [137/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [138/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [139/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [140/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [141/500], Step [100/100], Loss: 0.0001\n",
      "Epoch [142/500], Step [100/100], Loss: 0.0001\n",
      "Epoch [143/500], Step [100/100], Loss: 0.0003\n",
      "Epoch [144/500], Step [100/100], Loss: 0.0001\n",
      "Epoch [145/500], Step [100/100], Loss: 0.0001\n",
      "Epoch [146/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [147/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [148/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [149/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [150/500], Step [100/100], Loss: 0.0002\n",
      "Epoch [151/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [152/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [153/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [154/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [155/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [156/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [157/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [158/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [159/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [160/500], Step [100/100], Loss: 0.0001\n",
      "Epoch [161/500], Step [100/100], Loss: 0.0001\n",
      "Epoch [162/500], Step [100/100], Loss: 0.0001\n",
      "Epoch [163/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [164/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [165/500], Step [100/100], Loss: 0.0002\n",
      "Epoch [166/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [167/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [168/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [169/500], Step [100/100], Loss: 0.0004\n",
      "Epoch [170/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [171/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [172/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [173/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [174/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [175/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [176/500], Step [100/100], Loss: 0.0002\n",
      "Epoch [177/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [178/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [179/500], Step [100/100], Loss: 0.0001\n",
      "Epoch [180/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [181/500], Step [100/100], Loss: 0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [182/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [183/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [184/500], Step [100/100], Loss: 0.0001\n",
      "Epoch [185/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [186/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [187/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [188/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [189/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [190/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [191/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [192/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [193/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [194/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [195/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [196/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [197/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [198/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [199/500], Step [100/100], Loss: 0.0001\n",
      "Epoch [200/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [201/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [202/500], Step [100/100], Loss: 0.0001\n",
      "Epoch [203/500], Step [100/100], Loss: 0.0001\n",
      "Epoch [204/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [205/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [206/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [207/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [208/500], Step [100/100], Loss: 0.0001\n",
      "Epoch [209/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [210/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [211/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [212/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [213/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [214/500], Step [100/100], Loss: 0.0001\n",
      "Epoch [215/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [216/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [217/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [218/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [219/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [220/500], Step [100/100], Loss: 0.0002\n",
      "Epoch [221/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [222/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [223/500], Step [100/100], Loss: 0.0001\n",
      "Epoch [224/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [225/500], Step [100/100], Loss: 0.0001\n",
      "Epoch [226/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [227/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [228/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [229/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [230/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [231/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [232/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [233/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [234/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [235/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [236/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [237/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [238/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [239/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [240/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [241/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [242/500], Step [100/100], Loss: 0.0001\n",
      "Epoch [243/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [244/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [245/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [246/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [247/500], Step [100/100], Loss: 0.0002\n",
      "Epoch [248/500], Step [100/100], Loss: 0.0002\n",
      "Epoch [249/500], Step [100/100], Loss: 0.0001\n",
      "Epoch [250/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [251/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [252/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [253/500], Step [100/100], Loss: 0.0001\n",
      "Epoch [254/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [255/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [256/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [257/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [258/500], Step [100/100], Loss: 0.0003\n",
      "Epoch [259/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [260/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [261/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [262/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [263/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [264/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [265/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [266/500], Step [100/100], Loss: 0.0004\n",
      "Epoch [267/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [268/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [269/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [270/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [271/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [272/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [273/500], Step [100/100], Loss: 0.0001\n",
      "Epoch [274/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [275/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [276/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [277/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [278/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [279/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [280/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [281/500], Step [100/100], Loss: 0.0005\n",
      "Epoch [282/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [283/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [284/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [285/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [286/500], Step [100/100], Loss: 0.0001\n",
      "Epoch [287/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [288/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [289/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [290/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [291/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [292/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [293/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [294/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [295/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [296/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [297/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [298/500], Step [100/100], Loss: 0.0001\n",
      "Epoch [299/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [300/500], Step [100/100], Loss: 0.0001\n",
      "Epoch [301/500], Step [100/100], Loss: 0.0001\n",
      "Epoch [302/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [303/500], Step [100/100], Loss: 0.0001\n",
      "Epoch [304/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [305/500], Step [100/100], Loss: 0.0001\n",
      "Epoch [306/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [307/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [308/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [309/500], Step [100/100], Loss: 0.0002\n",
      "Epoch [310/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [311/500], Step [100/100], Loss: 0.0005\n",
      "Epoch [312/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [313/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [314/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [315/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [316/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [317/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [318/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [319/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [320/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [321/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [322/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [323/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [324/500], Step [100/100], Loss: 0.0001\n",
      "Epoch [325/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [326/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [327/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [328/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [329/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [330/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [331/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [332/500], Step [100/100], Loss: 0.0001\n",
      "Epoch [333/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [334/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [335/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [336/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [337/500], Step [100/100], Loss: 0.0001\n",
      "Epoch [338/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [339/500], Step [100/100], Loss: 0.0003\n",
      "Epoch [340/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [341/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [342/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [343/500], Step [100/100], Loss: 0.0001\n",
      "Epoch [344/500], Step [100/100], Loss: 0.0001\n",
      "Epoch [345/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [346/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [347/500], Step [100/100], Loss: 0.0001\n",
      "Epoch [348/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [349/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [350/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [351/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [352/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [353/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [354/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [355/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [356/500], Step [100/100], Loss: 0.0001\n",
      "Epoch [357/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [358/500], Step [100/100], Loss: 0.0001\n",
      "Epoch [359/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [360/500], Step [100/100], Loss: 0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [361/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [362/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [363/500], Step [100/100], Loss: 0.0001\n",
      "Epoch [364/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [365/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [366/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [367/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [368/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [369/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [370/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [371/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [372/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [373/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [374/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [375/500], Step [100/100], Loss: 0.0001\n",
      "Epoch [376/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [377/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [378/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [379/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [380/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [381/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [382/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [383/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [384/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [385/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [386/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [387/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [388/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [389/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [390/500], Step [100/100], Loss: 0.0002\n",
      "Epoch [391/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [392/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [393/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [394/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [395/500], Step [100/100], Loss: 0.0001\n",
      "Epoch [396/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [397/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [398/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [399/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [400/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [401/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [402/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [403/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [404/500], Step [100/100], Loss: 0.0001\n",
      "Epoch [405/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [406/500], Step [100/100], Loss: 0.0001\n",
      "Epoch [407/500], Step [100/100], Loss: 0.0001\n",
      "Epoch [408/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [409/500], Step [100/100], Loss: 0.0001\n",
      "Epoch [410/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [411/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [412/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [413/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [414/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [415/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [416/500], Step [100/100], Loss: 0.0001\n",
      "Epoch [417/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [418/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [419/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [420/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [421/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [422/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [423/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [424/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [425/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [426/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [427/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [428/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [429/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [430/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [431/500], Step [100/100], Loss: 0.0001\n",
      "Epoch [432/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [433/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [434/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [435/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [436/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [437/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [438/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [439/500], Step [100/100], Loss: 0.0001\n",
      "Epoch [440/500], Step [100/100], Loss: 0.0001\n",
      "Epoch [441/500], Step [100/100], Loss: 0.0002\n",
      "Epoch [442/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [443/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [444/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [445/500], Step [100/100], Loss: 0.0001\n",
      "Epoch [446/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [447/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [448/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [449/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [450/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [451/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [452/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [453/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [454/500], Step [100/100], Loss: 0.0001\n",
      "Epoch [455/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [456/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [457/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [458/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [459/500], Step [100/100], Loss: 0.0001\n",
      "Epoch [460/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [461/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [462/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [463/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [464/500], Step [100/100], Loss: 0.0003\n",
      "Epoch [465/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [466/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [467/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [468/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [469/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [470/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [471/500], Step [100/100], Loss: 0.0001\n",
      "Epoch [472/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [473/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [474/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [475/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [476/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [477/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [478/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [479/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [480/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [481/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [482/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [483/500], Step [100/100], Loss: 0.0001\n",
      "Epoch [484/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [485/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [486/500], Step [100/100], Loss: 0.0001\n",
      "Epoch [487/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [488/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [489/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [490/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [491/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [492/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [493/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [494/500], Step [100/100], Loss: 0.0001\n",
      "Epoch [495/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [496/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [497/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [498/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [499/500], Step [100/100], Loss: 0.0000\n",
      "Epoch [500/500], Step [100/100], Loss: 0.0000\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "epochs = 500\n",
    "batch_size = 100\n",
    "learning_rate = .0001\n",
    "\n",
    "net = myLSTM(1, 500, 1, 2)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "optimizer.zero_grad()\n",
    "\n",
    "\n",
    "#train\n",
    "running_loss = []\n",
    "for e in range(epochs):\n",
    "    for i in range(batch_size):\n",
    "        ind = random.randint(0,len(train_data)-80)\n",
    "        input = torch.FloatTensor(np.array([[price / 2500.0] for (date, price) in train_data[ind:ind+64]]) ).unsqueeze(0)\n",
    "        output = net(input)\n",
    "        \n",
    "        next_seven = [price for (date, price) in train_data[ind+64:ind+71]]\n",
    "        label = [0,0]\n",
    "        label[1] = np.max(next_seven) / 2500.0\n",
    "        label[0] = np.min(next_seven) / 2500.0\n",
    "        label = torch.FloatTensor([label])\n",
    "\n",
    "        # Backward and optimize\n",
    "#         print(input, output, label)\n",
    "        loss = criterion(output, label)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                   .format(e+1, epochs, i+1, batch_size, loss.item()))\n",
    "        running_loss.append(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1225.140015 [1192.3273 1226.3805]\n",
      "1253.069946 [1144.8141 1176.1786]\n",
      "1360.400024 [1303.3657 1338.3499]\n",
      "1386.52002 [1296.5803 1331.4149]\n",
      "1410.420044 [1300.296  1334.7051]\n",
      "1531.449951 [1458.986  1496.6853]\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "ind = [0, 50, 100, 150, 200, 250]\n",
    "\n",
    "net.eval()\n",
    "with torch.no_grad():\n",
    "    for i in ind:\n",
    "        graph_set = [price for (date, price) in test_data[i:i+71]]\n",
    "        test_graph = [price for (date, price) in test_data[i:i+64]]\n",
    "        test_graph_label = [price for (date, price) in test_data[i+64:i+71]]\n",
    "\n",
    "        max_test = np.max(test_graph_label)\n",
    "        min_test = np.min(test_graph_label)\n",
    "        output = net(torch.Tensor(np.array([[price/2500] for (date, price) in test_data[i:i+64]]) ).unsqueeze(0))\n",
    "\n",
    "        output = output.detach().numpy()[0] * 2500\n",
    "        print(max_test, output)\n",
    "        # # next seven\n",
    "        # for price in test_data[ind+64:ind+71]:\n",
    "        #     graph_set.append(price[1])\n",
    "\n",
    "        plt.plot(graph_set)\n",
    "        plt.plot(test_graph)\n",
    "        # last seven\n",
    "    #     plt.axline((64,.5),(64,.6), c='green')\n",
    "        # predicted max\n",
    "        plt.axline((0,output[0]),(1,output[0]), c='red')\n",
    "        plt.axline((0,output[1]),(1,output[1]), c='red')\n",
    "        # actual max\n",
    "        plt.axline((0,max_test),(1,max_test), c='orange')\n",
    "        plt.axline((0,min_test),(1,min_test), c='orange')\n",
    "\n",
    "        plt.savefig('output/'+str(i)+'_stockspred.jpg')\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
